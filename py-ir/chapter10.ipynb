{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第10章 表現学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# これまでに定義した関数の読み込み\n",
    "\n",
    "from chapter01 import get_string_from_file\n",
    "from chapter02 import get_words_from_file, configure_fonts_for_japanese\n",
    "from chapter03 import get_words, bows_to_cfs, load_aozora_corpus, get_bows,  add_to_corpus,\\\n",
    "    get_weights, translate_bows, get_tfidfmodel_and_weights\n",
    "from chapter04 import vsm_search, get_list_from_file\n",
    "from chapter05 import top_n, get_pr_curve, get_average_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data =  [['年越し', 'に', 'は', '天ぷら', '蕎麦', 'を', 'いただき', 'ます'], ['関東', 'で', 'は', '雑煮', 'に', '鶏肉', 'と', '小松菜', 'を', '入れ', 'ます']]\n",
      "蕎麦 = [-0.13285957 -0.03071345 -0.03037666]\n"
     ]
    }
   ],
   "source": [
    "# Listing 10.1 #\n",
    "\n",
    "# 学習用の文書データ\n",
    "training_documents = ['年越しには天ぷら蕎麦をいただきます',\n",
    "                                   '関東では雑煮に鶏肉と小松菜を入れます']\n",
    "\n",
    "# 名詞の並びに変換\n",
    "training_data = [get_words(d) for d in training_documents]\n",
    "print('training_data = ', training_data)\n",
    "\n",
    "# ライブラリの読み込み\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 埋め込みを計算\n",
    "w2v_model = Word2Vec(training_data, size=3, window=2, sg=1, min_count=1)\n",
    "\n",
    "# ベクトルの表示\n",
    "word = '蕎麦'\n",
    "print(word, '=', w2v_model.wv[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 10.2 #\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word_vectors =KeyedVectors.load('data/ch10/w2v.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('吸入', 0.8519949913024902),\n",
      " ('ヘモグロビン', 0.8410395383834839),\n",
      " ('水素', 0.837383508682251),\n",
      " ('熱量', 0.8194736242294312),\n",
      " ('試験管', 0.8179594278335571)]\n"
     ]
    }
   ],
   "source": [
    "# Listing 10.3 #\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(word_vectors.most_similar(positive=['酸素'],topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('スパゲッティ', 0.5985097289085388)]\n"
     ]
    }
   ],
   "source": [
    "# Listing 10.4 #\n",
    "\n",
    "print(word_vectors.most_similar(positive=['饂飩', 'イタリア'], negative=['日本'], topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "饂飩 - 日本 + イタリア = スパゲッティ\n",
      "信長 - 尾張 + 美濃 = 道三\n",
      "丸の内 - 東京 + 大阪 = 堂島\n"
     ]
    }
   ],
   "source": [
    "# Listing 10.5 #\n",
    "\n",
    "data = [[['饂飩', 'イタリア'], ['日本']],\n",
    "            [['信長', '美濃'], ['尾張']],\n",
    "            [['丸の内', '大阪'], ['東京']]]\n",
    "\n",
    "for p_words, n_words in data:\n",
    "    top = word_vectors.most_similar(positive=p_words, negative=n_words, topn=1)\n",
    "    \n",
    "    # [('スパゲッティ', 0.5985)] から'スパゲッティ'を取り出す\n",
    "    answer = top[0][0]\n",
    "    \n",
    "    print('{} - {} + {} = {}'.format(p_words[0], n_words[0], p_words[1], answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## 10.4 Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['年越し', 'に', 'は', '天ぷら', '蕎麦', 'を', 'いただき', 'ます'], tags=[0]),\n",
      " TaggedDocument(words=['関東', 'で', 'は', '雑煮', 'に', '鶏肉', 'と', '小松菜', 'を', '入れ', 'ます'], tags=[1])]\n"
     ]
    }
   ],
   "source": [
    "# Listing 10.6 #\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "tagged_data = [TaggedDocument(words=d, tags=[i]) for i, d in enumerate(training_data)]\n",
    "\n",
    "pprint(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 10.7 #\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# Doc2vecのモデルを作成\n",
    "d2v_model = Doc2Vec(tagged_data, dm=1, vector_size=3, window=2, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.14809698 -0.01563     0.02351491]\n"
     ]
    }
   ],
   "source": [
    "# Listing 10.8 #\n",
    "\n",
    "# タグ0を付与した文書のベクトルを表示\n",
    "print(d2v_model.docvecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 10.9 #\n",
    "\n",
    "# ダウンロードした学習済みモデルのファイル\n",
    "# (ダウンロードしたファイルをディレクトリdataに置いた場合)\n",
    "model_file = 'data/jawiki.doc2vec.dmpv300d.model'\n",
    "\n",
    "# モデルの読み込み(時間がかかる)\n",
    "d2v_wikipedia_model = Doc2Vec.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc1 = ['Python', 'を', '使う', 'と', '自然', '言語', '処理', 'が', '簡単', 'に', 'できる']\n",
      "doc2 = ['実', 'データ', 'を', '用い', 'た', '情報', '検索', 'プログラミング', 'は', '楽しい']\n",
      "類似度: 0.5479 \n"
     ]
    }
   ],
   "source": [
    "# Listing 10.10 #\n",
    "\n",
    "# テキストを語の並びに変換\n",
    "doc1 = get_words('Pythonを使うと自然言語処理が簡単にできる')\n",
    "doc2 = get_words('実データを用いた情報検索プログラミングは楽しい')\n",
    "print('doc1 = {}'.format(doc1))\n",
    "print('doc2 = {}'.format(doc2))\n",
    "\n",
    "# 学習済みモデルに基づくdoc1とdoc2の類似度の計算\n",
    "print('類似度: %.4f ' % \n",
    "      d2v_model.docvecs.similarity_unseen_docs(d2v_wikipedia_model, doc1, doc2, steps=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 10.11 #\n",
    "\n",
    "def d2v_search(model, texts, query):\n",
    "    # textsの各要素を名詞の並びに変換\n",
    "    docs = [get_words(text) if type(text) is str else text for text in texts]\n",
    "\n",
    "    # queryも変換\n",
    "    query_doc = get_words(query) if type(query) is str else query\n",
    "\n",
    "    # docs[i]とquery_docをベクトル化して類似度を計算\n",
    "    # そして，(i, 類似度) をi番目の要素とするリストを作成\n",
    "    r = [(i, model.docvecs.similarity_unseen_docs(model, doc,  query_doc, steps=50))\n",
    "             for i, doc in enumerate(docs)]\n",
    "    # 類似度に関して降順にソートしたもの（ランキング）を返す\n",
    "    return sorted(r, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4, 8, 2, 0, 1, 9, 5, 6, 7)\n",
      "0.6335\n"
     ]
    }
   ],
   "source": [
    "# Listing 10.12 #\n",
    "\n",
    "# 5章のデータの読み込み\n",
    "book_texts = [get_string_from_file('data/ch05/%d.txt' % i) for i in range(10)]  \n",
    "\n",
    "# クエリ\n",
    "query = '人工知能'\n",
    "\n",
    "# '人工知能'で検索した場合の正解（5章）\n",
    "right_answer = [0, 1, 0, 1, 0, 1, 0, 0, 1, 1]\n",
    "\n",
    "# 検索の実行\n",
    "result = d2v_search(d2v_wikipedia_model, book_texts, query)\n",
    "\n",
    "# 検索結果（ランキング）の表示\n",
    "ranking = tuple([x[0] for x in result])\n",
    "print(ranking)\n",
    "\n",
    "# 平均適合率の計算と表示\n",
    "print('%.4f' % get_average_precision(ranking, right_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8, 9, 5, 3, 0, 6, 2, 4, 7)\n",
      "1.0000\n"
     ]
    }
   ],
   "source": [
    "# Listing 10.13 #\n",
    "\n",
    "import pickle\n",
    "with open('data/ch10/tokenized.dat', 'rb') as f:\n",
    "    tokenized_texts, tokenized_query = pickle.load(f)\n",
    "\n",
    "result = d2v_search(d2v_wikipedia_model, tokenized_texts, tokenized_query)\n",
    "\n",
    "ranking = tuple([x[0] for x in result])\n",
    "print(ranking)\n",
    "print('%.4f' % get_average_precision(ranking, right_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
